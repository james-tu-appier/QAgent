# QAgent: RAT Test Case Generator

## Overview

QAgent is a test case generator that takes in context of a feature and generates relevant RAT test cases.

### Disclaimers: 
This tool does NOT guarantee high-quality test cases or maximum coverage (I wouldn't trust any tool that claims it does), so human review is needed to filter/add/modify test cases generated by QAgent. The tool follows the garbage-in-garbage-out rule, so having good documentation or description of the feature under test is critical for good output. 

The current version aims to strike a balance between generating enough test cases to cover critical paths and edge cases while also keeping the amount of test cases low enough to make the time needed for human review reasonable. 

There are still parts of the code that are a WIP, but feel free to look around the codebase. (Code contributions are very welcome! )


### Strengths: 

- **Simple:** Quick and easy-to-use pipeline.

- **Stable:** Structured output for easier utilization. 

- **Highly versatile:** Replace [prompts](./backend/prompt_templates/), models, or output structure according to your needs. 

- **Cost Efficient:** With moderate usage, the tool can be entirely free (if you use the free gemini api key for gemini-2.5-flash).

- **Utility-Oriented:** With your generated test cases, you can easily upload them to testrail for future use. In addition, I have also provided a utility tool that allows you to upload .mm files from mindmeister to Testrail [here](./utils/). You can also load results from previous sessions. 

> Tip: This tool can be used in sync with other AI tools, e.g. given the .json file of generated test cases, feel free to allow other AI tools to fix the test cases for you. 

### Use cases: 
1. Serves as a pretty good step 0 for QAs' test case design tasks. (QAgent generates -> QAs review and add/remove/modify)

2. For scenarios where QA resources are strained, this tool can be utilized by devs to generate RAT test cases for QA review. (Devs use QAgent to generate test cases -> Devs filter and review test cases -> QA reviews final version)

### Future directions: (if there is high demand)

- Adding functionality to output the test cases to mindmeister for easier review and modification.

- Enhancing the context retrieval pipeline to use JIRA or Confluence MCP to get relevant context instead of the current vanilla document upload function. 

- Fine-tune prompts to strike a better balance for utility, (or better yet), build a prompt library so that there are different prompts for different use cases. 

- Extend support for other models. 

### Contributing
My main goal is to help reduce QA workload without sacrificing quality so help me help QAgent help you! Feel free to ping me on Slack (James Tu) All ideas are welcome and let me know if you're interested in contributing! 

## Quick Start

```bash
# 1) Clone
git clone <this-repo>
cd qagent

# 2) Create and activate a virtualenv (recommended)
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 3) Install dependencies
pip install -r requirements.txt

# 4) Run the app
python app.py

# 5) Open in browser
# http://localhost:8080
```

### Requirements

- Python 3.9+ recommended
- Optional: Google Gemini API key and Figma access token for full functionality

### Environment Variables

Create a `.env` in the repo root:

```env
# Required for full (non-demo) mode
GEMINI_API_KEY="your_gemini_api_key"
FIGMA_ACCESS_TOKEN="your_figma_access_token"

# For TestRail upload
TESTRAIL_URL=https://appier.testrail.io/
TESTRAIL_USER="your_user_email"
TESTRAIL_PASSWORD_OR_KEY="your_password_or_api_key"
```

Notes:
- If `GEMINI_API_KEY` or `FIGMA_ACCESS_TOKEN` are missing, the app will run in demo mode with mock data.

### Running the Frontend

- Start the Flask app:
  ```bash
  python app.py
  ```
- Visit `http://localhost:8080`
- Upload a PRD file (`.pdf`, `.txt`, `.md`), paste a Figma URL, and optionally toggle Trust Mode:
  - Trust Mode: automatic, end-to-end generation
  - Checkpoint Mode: review and edit at key steps

Outputs are saved under `output/<session_id>/`:
- `prd_context.json`, `figma_summary.txt`
- `test_plan.json`, `test_plan.md`
- `test_suite.json`, `test_suite.md`

### Post to TestRail!! 

Once a plan is generated, you can POST the results to an existing Testrail Test Suite by providing: 
```json
{ "project_id": x, "suite_id": xxxxxx }
```
Requires `TESTRAIL_URL`, `TESTRAIL_USER`, `TESTRAIL_PASSWORD_OR_KEY` in `.env`.

### Troubleshooting
Contact james.tu@appier.com 

### Project Structure (high-level)

- `app.py`: Flask app and routes
- `templates/`, `static/`: frontend UI
- `backend/`: modular classes for PRD extraction, Figma parsing, plan generation
- `uploads/`: uploaded PRDs
- `output/`: generated artifacts by session

### Notes: 
1. If you wish to load results from previous sessions, use the session id from the output folder and enter it in the View Previous Session section on the homepage. 
2. If you want the test cases to focus on certain aspects or modify the coverage of testing types, feel free to update the backend/prompt_templates to best fit your needs. (If you come across great results after prompt engineering, please share your findings with me!!)


